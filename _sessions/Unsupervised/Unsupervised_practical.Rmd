---
title: "Unsupervised learning"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Machine learning with R</font><br>
      <a href='https://therbootcamp.github.io/ML-DHLab'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>The R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = TRUE, 
                      eval = TRUE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)

set.seed(100)
```

```{r, eval = TRUE, echo = FALSE}
# Load datasets locally
library(tidyverse)
library(cstab)
library(dbscan)
library(mclust)
gap = gapminder::gapminder %>% filter(country != 'Kuwait')

```
<p align="center">
<img width="100%" src="image/gapminder_banner.png" margin=0><br>
<font style="font-size:10px">from [gapminder.org](https://www.gapminder.org/data/)</font>
</p>

# {.tabset}

## Überblick

In this practical, you will learn how to apply cluster analysis to two data sets.

In the end, you will know how to:

1. How to identify clusterings using different algorithms. 
2. How to estimate the number of clusters for a given problem. 

## Aufgaben

### A - Setup

1. Open your `TheRBootcamp` R project. 

2. Open a new R script. Write your name, the date and "Unsupervised learning Practical" as comments at the top. 

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATUM
## Unsupervised learning practical
```

3. Save the script as `Unsupervised_practical.R` in the `2_Code` folder.

4. Load the packages `tidyverse`, `cstab`, `dbscan`, and `mclust`.

### B - Load the `gap` data set

1. Using `read_csv()`, read in `gap.csv` and save it as `gap`.

```{r, echo = T}
# Read gap.csv
gap <- read_csv('1_Data/gap.csv')
```

2. Print the data set and inspect its contents. 

3. Use `summary()` to get additional insight into the data.

4. Use the code below to create a new data set containing only the data from year 2007 and features `Lebenserwartung` and `BIP pro Kopf`.

```{r, echo = T}
# gap in 2007
gap2007 <- gap %>% 
  filter(Jahr == 2007) %>% 
  select(`BIP pro Kopf`, Lebenserwartung)
```

### C - *k*-means

1. Using `kmeans()`, identify three clusters (`centers`) in `gap2007`. 

```{r, echo = T, eval = F}
# kmeans of gap in 2007
gap2007_km <- kmeans(x = XX, centers = XX) 
```

```{r}
# kmeans of gap in 2007
gap2007_km <- kmeans(x = gap2007, centers = 3) 
```

2. Print `gap2007_km` and study the outputt.

```{r}
# kmeans of gap in 2007
gap2007_km
```

3. The first row of the ouput and the table `Cluster means` shows how many objects were assigned to each of the three clusters and where the centroids (cluster means) of these are located.

4. At the bottom of the output is a list of names of objects contained in the clustering object.  Use `gap2007_km$XX` to select the object `clusters`, as well as the elements `centers` and store them as `clusters` and `centers` respectively.

```{r, echo = T, eval = F}
# gap2007_km 
clusters <- gap2007_km$XX
centers <- gap2007_km$XX
```

```{r}
# gap2007_km 
clusters <- gap2007_km$cluster
centers <- gap2007_km$centers
```

5. Use the code below to plot the data and cluster assignments. 

```{r, echo = T}
# kmeans of gap in 2007
plot(gap2007, col = clusters)
```

6. Now use the code below to add the centroids. 

```{r, echo = T}
# kmeans of gap in 2007
plot(gap2007, col = clusters)
points(centers, pch = 16, col = 1:3, cex = 2)
```

7. Something's off, right? Some points of the middle cluster actually seem to lie closer to the bottom-left cluster. This shouldn't be the case. Any ideas how this has come about?

8. The problem is that the features have different scales. The values of `BIP pro Kopf` are way larger than those in `Lebenserwartung` and, thus, a lot further away from each other. For that reason, `BIP pro Kopf` plays a much larger role for cluster assignments than `Lebenserwartung`. To fix this problem, run the code below, which scaled the features in `gap2007`. 

```{r, echo = T}
# scale gap in 2007
gap2007_scaled <- gap2007 %>% 
  scale() %>% 
  as_tibble()
```

9. Now, run `kmeans()` for `gap2007_scaled` and plot the data and cluster assignments. All good now?

```{r}
# kmeans plot for gap in 2007 
gap2007_scaled_km <- kmeans(x = gap2007_scaled, centers = 3) 

# extract elements
clusters <- gap2007_scaled_km$cluster
centers <- gap2007_scaled_km$centers

# plot
plot(gap2007_scaled, col = clusters)
points(centers, pch = 16, col = 1:3, cex = 2)
```

### D - *k*-selection

1. It's time to try to estimate how many clusters there might be in the data. Use the code below to create a vector of within-cluster variances for `kmeans` solutions associated with 2 to 20 clusters. The code uses the `gap2007_scaled` data.

```{r, echo = T}
# within-cluster variance development
km_development <- purrr::map(2:20, kmeans, x = gap2007_scaled)
withinvar <- purrr::map_dbl(km_development, 
                            `[[`, i = 'tot.withinss')
```

2. Using `plot()` create a plot of the development of the `withinvar`.

```{r}
# kmeans within-variance development
plot(withinvar)
```

3. What does the plot tell you? Is there an elbow that would suggest a particular value of *k*?

4. Several values of *k* seem plausible: 1, 3, or 7. Use `cDistance()` from the `cstab` package to estimate *k* with values from 2 to 20 (`2:20`) as candidates.

```{r, echo = T, eval = F}
# estimate k with cstab
k_est <- cDistance(data = as.matrix(XX),
                   kseq = XX:XX)
```

```{r}
# estimate k with cstab
k_est <- cDistance(data = as.matrix(gap2007_scaled),
                   kseq = 2:20)
```

5. Extract `k_est$k_Gap` und `k_est$k_Slope`. Do the numbers seem reasonable?

```{r}
# estimate k with cstab
k_est$k_Gap
k_est$k_Slope
```

6. Now try `cStability()` and extract `k_instab`. Reasonable?

```{r}
# estimate k with cstab
k_est <- cStability(data = as.matrix(gap2007_scaled),
                   kseq = 2:20)
k_est$k_instab
```

Remember: There is no true *k*.

### E - DBSCAN

1. Use `dbscan()` from the `dbscan` package to cluster the data. Again it is essential to work with `gap2007_scaled` as otherwise `eps` would describe an ellipse and not a circle. Set `eps` to `.5`. 

```{r, echo = TRUE, eval = FALSE}
# clustering with DBSCAN
gap2007_scaled_dbscan <- dbscan(x = XX, 
                                eps = XX)
```

```{r}
# clustering with DBSCAN
gap2007_scaled_dbscan <- dbscan(x = gap2007_scaled, 
                                eps = .5)
```

2. Printe `gap2007_scaled_dbscan`. What does the outpute tell you?

...

3. Was verrät euch der Output? Erinnert euch ein Cluster von 0 bedeutet Outlier.  

4. Ein einzelner Cluster und 5 Outlier wurden identifiziert. Schaut euch das Ergenis an indem ihr wie oben das Element `cluster` extrahiert und dann die Daten mit eingefärbten Clustern plottet. Das `+ 1` ist notwendig, weil ein Wert von `0` keine Farbe bedeutet. 

```{r, echo = T, eval = F}
# extrahiere Elemente
clusters <- gap2007_scaled_dbscan$XX

# plot
plot(XX, col = XX + 1)
```

```{r}
# extrahiere Elemente
clusters <- gap2007_scaled_dbscan$cluster

# plot
plot(gap2007_scaled, col = clusters + 1)
```

5. Lasse `dbscan()` erneut laufen, aber mit anderen Werten für `eps`. Versuche `eps = .3` und `eps = .1` und plotte jeweils das Ergebnis. Ändert sich was?

```{r}
# clustere mit DBSCAN
gap2007_scaled_dbscan.3 <- dbscan(x = gap2007_scaled, eps = .3)
gap2007_scaled_dbscan.1 <- dbscan(x = gap2007_scaled, eps = .1)

# plot
par(mfrow = c(1, 3))
plot(gap2007_scaled, col = gap2007_scaled_dbscan$cluster + 1)
plot(gap2007_scaled, col = gap2007_scaled_dbscan.3$cluster + 1)
plot(gap2007_scaled, col = gap2007_scaled_dbscan.1$cluster + 1)
```

6. `dbscan` hat einen weiteren Parameter `minPts`, welcher bestimmt, wie viele Punkte in einem Abstand von `eps` liegen müssen, damit der Punkt ein Kernpunkt wird. Versuche ein paar verschiedene Werte und versuche zu verstehen was passiert.

### F - Gaussian Mixtures

1. Zum Abschluss, verwende `Mclust()` aus dem `mclust` Paket um über Gaussian Mixture Modelle die Cluster zu bestimmen. Arbeite hier mit dem nicht-standardisierten Datensatz `gap2007`. Dies ist möglich, weil Gaussian Mixtures die Skalen der Variablen automatisch berücksichtigt. 

```{r, echo = TRUE, eval = FALSE}
# clustere mit Gaussian mixtures
gap2007_gm <- Mclust(XX)
```

```{r, error=TRUE}
# clustere mit Gaussian mixtures
gap2007_gm <- Mclust(gap2007)
```

2. Printe das Objekt `gap2007_gm` um es zu inspizieren.

3. Der Output verrät relativ wenig, nur welche Elemente enthalten sind. Verwende `table(gap2007_gm$classification)` um einen Überblick über die Clusterzuweisungen zu erhalten. Wie viele Cluster wurden identifiziert?    

4. Verwende das `classification` Element um wie üblich die Daten mit den Clusterzuweisungen zu plotten. 

```{r}
# plot
plot(gap2007_scaled, col = gap2007_gm$classification)
```

5. Führe nun alternativ `plot(gap2007_gm, what = 'classification')` aus, um den eigenen Plot des `mclust` Pakets zu sehen. 

```{r}
# plot
plot(gap2007_gm, what = 'classification')
```

6. Versuche nachzuvollziehen was der `mclust` Plot euch zeigt. Erinnere dich, die Ellipsen sind Normalverteilungen, die jeweils eigene Skalen und Feature-Zusammenhänge berücksichtigen können.  

7. Eine interessante Eigenschaft von Gaussian Mixtures ist, dass man direkt die Unsicherheit der Clusterzuweisung evaluieren kann. Führe `plot(gap2007_gm, what = 'uncertainty')` aus. Die Grössen der Punkte zeigen an, wie gross die Unsicherheit (oder Rivalität) in der Zuweisung der Punkte zu den Clustern war. 

```{r}
# plot
plot(gap2007_gm, what = 'uncertainty')
```

### X - Challenges: Modellselektion Gaussian mixtures

1. Eine nützliche Eigenschaft der `Mclust()` Funktion ist, dass parallel verschieden komplexe Varianten des Modells mit verschiedenem *k* geschätzt werden und dass am Ende nicht nur das beste *k* ausgewählt wird, sondern die beste Kombination von *k* und Modell. Du erhälst einen Überblick über den Prozess mit `plot(gap2007_gm, what = 'BIC')`.

```{r}
# plot
plot(gap2007_gm, what = 'BIC')
```

BIC ist das sogenannte Bayesian Information Criterion und dient der Auswahl eines Modells unter Berücksichtigung der Komplexität des Modells. In diesem Fall sind hohe Werte besser. In der Abbildung siehst du nun wie sich der BIC Wert über verschiedene *k* (Number of components) und Modelle (verschiedene Linien) entwickelt. 

2. Verwende `plot(gap2007_gm, what = 'BIC', ylim = c(-4200, -3900))` um einen besseren Ausschnitt zu erhalten. Nun solltest du sehen können, dass das `EVV` Modell den besten BIC für 4 Komponenten erzielt. Entsprechend wurde dieses Modell ausgewählt. 

```{r}
# plot
plot(gap2007_gm, what = 'BIC', ylim = c(-4200, -3900))
```

3. Lasse dir mit `?mclustModelNames` die Erläuterung zu den Modellbezeichnungen anzeigen. Dort findest zu heraus, dass das Modell annimmt, dass das Volumen der einzelnen Cluster (hier die Fläche der Ellipsoide) gleich ist. Dies lässt sich auch in `plot(gap2007_gm, what = 'classification')` erkennen.  

```{r}
plot(gap2007_gm, what = 'classification')
```


4. Verwende nun den Code unten um explizit nur bestimmte Gaussian Mixture Modelle zu verwenden. Verwende hierzu `modelNames = 'XX'` wobei 'XX' das Kürzel des jeweiligen Modells ist. Plotte im Anschluss die gefunden Lösungen. Probiere zunächst `EEI` aus. Danach spiele ein wenig herum.

```{r, echo = T, eval = F}
# Wähle Gaussian Mixture Modell explizit aus
gap2007_gm <- Mclust(gap2007, modelNames = 'XX')
plot(gap2007_gm, what = 'classification')
```

```{r}
# Wähle Gaussian Mixture Modell explizit aus
gap2007_gm <- Mclust(gap2007, modelNames = 'EEI')
plot(gap2007_gm, what = 'classification')
```


### Y - Challenges: Neuer Datensatz

1. Verwende die `read_csv()` Funktion um den Datensatz `credit.csv` als Objekt `credit` einzulesen.

```{r, echo = T, eval = T, message = F}
# Lese credit.csv
credit <- read_csv('1_Data/credit.csv')
```

2. Printe den Datensatz und verwende `summary()` um einen weiteren Überblick über die Daten zu bekommen.

3. Verwende die bis hierin geübten Methoden um zu identifizieren, ob und wie viele Cluster sich im `credit` Datensatz befindet. Bzw. ob und wie sich Kreditkarten Kunden in Gruppen zusammenfassen lassen. Viel Spass!


## Beispiele

```{r, eval = FALSE, echo = TRUE}
library(tidyverse) 
library(cstab)
library(dbscan)
library(mclust, mask.ok = F)

# Beispieldatensatz
data(mpg)

# Verarbeitung des Datensatzes
mpg <- mpg %>% select_if(is.numeric)
mpg_stand <- mpg  %>% 
  scale %>%         # Standardisieren
  as_tibble()

# k-means -----

# Finde Cluster
mpg_km <- kmeans(mpg_stand, 
                 centers = 3)

# Zeige Zentroide
mpg_km$centers

# k-selection -----

# Zeige Binnenvarianz Verlauf
km_verlauf <- purrr::map(2:20, kmeans, x = mpg_stand)
binnenvarianz <- purrr::map_dbl(km_verlauf, 
                               `[[`, i = 'tot.withinss')

# Plotte die Binnenvarianz
plot(binnenvarianz)

# Gap & Slope Statistik
k_est <- cDistance(as.matrix(mpg_stand), 
                   kseq = 2:20) 
k_est$k__Gap
k_est$k_Slope

# Cluster stability
k_est <- cStability(as.matrix(mpg_stand), 
                    kseq = 2:20) 
k_est$k_instab
  
# DBSCAN -----

# Finde Cluster
mpg_dbscan <- dbscan(mpg_stand, eps = 1)

# Zeige Zentroide
mpg %>% 
  mutate(cl = mpg_dbscan$cluster) %>%
  group_by(cl) %>% 
  summarize_all(mean)

# Gaussian Mixtures -----

# Finde Cluster
mpg_gm <- Mclust(mpg)

# Zeige Zentroide
mpg %>% 
  mutate(cl = mpg_gm$classification) %>%
  group_by(cl) %>% 
  summarize_all(mean)

# Plotte Cluster
plot(mpg_gm, what = 'classification')

# Vergleiche Cluster -----

table(mpg_km$cluster, mpg_dbscan$cluster)
table(mpg_km$cluster, mpg_gm$classification)
table(mpg_dbscan$cluster, mpg_gm$classification)

```


## Datensätze

|Datei | Zeilen | Spalten | 
|:----|:-----|:------|
|[gap.csv](https://raw.githubusercontent.com/therbootcamp/ML_2020Apr/master/_sessions/Unsupervised/1_Data/gap.csv) | 1692 | 6 | 
|[credit.csv](https://raw.githubusercontent.com/therbootcamp/ML_2020Apr/master/_sessions/Unsupervised/1_Data/credit.csv) | 8636 | 8 | 

#### gap.csv

Der `gap` Datensatz basiert auf dem [Gapminder](https://www.gapminder.org/) Projekt und stammt aus dem R Paket [gapminder](https://cran.r-project.org/web/packages/gapminder/README.html).  

|Variable | Beschreibung |
|:-------------|:-------------------------------------|
|Land| Name of country  |
|Kontinent| Name of continent |
|Jahr| year |
|Lebenserwartung| life expectancy in years |
|Population| Anzahl Population of country |
|BIP pro Kopf| GDP per capita |


#### credit.csv

Der `credit` Datensatz ist ein Ausschnitt des Öffentlich verfügbaren [*Credit Card Dataset*](https://www.kaggle.com/arjunbhasin2013/ccdata). Der Datensatz beinhaltet 8 Features, die einen Auschnitt des Verhaltens von 8636 Kreditkartenkunden beschreiben.  

|Variable | Beschreibung |
|:-------------|:-------------------------------------|
|BALANCE| Verfügbares Guthaben  |
|BALANCE_FREQUENCY| Änderungsfrequenz des Guthabens (1 = häufig, 0 = selten) |
|PURCHASES| Summe der Einkäufe |
|CREDITLIMIT| Kreditlimit der Karte |
|ONEOFFPURCHASES| Betrag der grössten einmaligen Zahlung |
|MINIMUM_PAYMENTS| Minimale Konto-Ausgleichszahlung  |
|PRCFULLPAYMENT| Prozent vollständige Konto-Ausgleichszahlung  |
|TENURE| Dauer des Kundenverhältnisses   |





## Funktionen

### Paket

|Paket| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`cstab`|`install.packages("cstab")`|
|`dbscan`|`install.packages("dbscan")`|
|`mclust`|`install.packages("mclust")`|

### Funktionen

*Clustering*

| Funktion| Paket | Beschreibung |
|:---|:------|:---------------------------------------------|
| `kmeans()`|`stats`| Clustere die Daten mit *k*-means | 
| `dbscan()`|`dbscan`| Clustere die Daten mit DBSCAN | 
| `Mclust()`|`mclust`| Clustere die Daten mit Gaussian Mixtures | 

*k-selection*

| Funktion| Paket | Beschreibung |
|:---|:------|:---------------------------------------------|
| `cDistance()`|`cstab`| Identifiziere *k* mit distanzbasierten Methoden, z.B., der Gap Statistik.  | 
| `cStability()`|`cstab`| Identifiziere *k* mit stabilitätsbasierten Methoden. | 


## Materialien

### Dokumentation

- Eine gutes [**Tutorial**](https://www.r-bloggers.com/the-complete-guide-to-clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/) über *k*-means und hierarchisches Clustering.

