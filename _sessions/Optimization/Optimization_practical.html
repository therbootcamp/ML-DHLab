<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Optimization</title>

<script src="Optimization_practical_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Optimization_practical_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Optimization_practical_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Optimization_practical_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Optimization_practical_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="Optimization_practical_files/navigation-1.1/tabsets.js"></script>
<link href="Optimization_practical_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Optimization_practical_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="practical.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimization</h1>
<h4 class="author"><table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'>
<col width='10%'>
<col width='10%'>
<tr style="border:none">
<td style="display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none" nowrap>
<font style='font-style:normal'>Machine Learning with R</font><br> <a href='https://therbootcamp.github.io/ML-DHLab/'> <i class='fas fa-clock' style='font-size:.9em;' ></i> </a> <a href='https://therbootcamp.github.io'> <i class='fas fa-home' style='font-size:.9em;'></i> </a> <a href='mailto:therbootcamp@gmail.com'> <i class='fas fa-envelope' style='font-size: .9em;'></i> </a> <a href='https://www.linkedin.com/company/basel-r-bootcamp/'> <i class='fab fa-linkedin' style='font-size: .9em;'></i> </a> <a href='https://therbootcamp.github.io'> <font style='font-style:normal'>The R Bootcamp</font> </a>
</td>
<td style="width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none">
<img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
</td>
</tr>
</table></h4>

</div>


<p align="center">
<img width="100%" src="image/rexthor.png" margin=0><br> <font style="font-size:10px">from <a href="https://xkcd.com/1725/">xkcd.com</a></font>
</p>
<div id="section" class="section level1 tabset">
<h1></h1>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>By the end of this practical you will know how to:</p>
<ol style="list-style-type: decimal">
<li>Use cross-validation to select optimal model tuning parameters for decision trees and random forests.</li>
<li>Compare ‘standard’ regression with lasso and ridge penalised regression.</li>
</ol>
</div>
<div id="tasks" class="section level2">
<h2>Tasks</h2>
<div id="a---setup" class="section level3">
<h3>A - Setup</h3>
<ol style="list-style-type: decimal">
<li><p>Open your <code>TheRBootcamp</code> R project.</p></li>
<li><p>Open a new R script. At the top of the script, using comments, write your name and the date.</p></li>
</ol>
<pre class="r"><code>## NAME
## DATE
## Optimizing practical</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><p>Save the script as a new file called <code>Optimization_practical.R</code> in the <code>2_Code</code> folder.</p></li>
<li><p>Using <code>library()</code> load the packages <code>tidyverse</code>, <code>caret</code>, <code>party</code>, <code>partykit</code>.</p></li>
</ol>
<pre class="r"><code># Load packages necessary for this script
library(tidyverse)
library(caret)
library(party)
library(partykit)</code></pre>
</div>
<div id="b---load-the-graduation-data" class="section level3">
<h3>B - Load the <code>graduation</code> data</h3>
<ol style="list-style-type: decimal">
<li>You will again begin by analyzing the graduation data. Read in the data sets <code>graduation_train.csv</code> and <code>graduation_test.csv</code> and convert all character to factors.</li>
</ol>
<pre class="r"><code># Read college data
college_train &lt;- read_csv(file = &quot;1_Data/college_train.csv&quot;)
college_test &lt;- read_csv(file = &quot;1_Data/college_test.csv&quot;)

# Convert all character features to factor
college_train &lt;- college_train %&gt;%
  mutate_if(is.character, factor) 
college_test &lt;- college_test %&gt;%
          mutate_if(is.character, factor)</code></pre>
</div>
<div id="c---setup-traincontrol" class="section level3">
<h3>C - Setup <code>trainControl</code></h3>
<ol style="list-style-type: decimal">
<li>Now, you finally make use of the train control object by specifying 10-fold cross-validation as the preferred optimization method in an object called <code>ctrl_cv</code>. Specifically:</li>
</ol>
<ul>
<li>set <code>method = "cv"</code> to specify cross validation.</li>
<li>set <code>number = 10</code> to specify 10 folds.</li>
</ul>
<pre class="r"><code># Use 10-fold cross validation
ctrl_cv &lt;- trainControl(method = &quot;XX&quot;, 
                        number = XX) </code></pre>
<pre class="r"><code># Use 10-fold cross validation
ctrl_cv &lt;- trainControl(method = &quot;cv&quot;, 
                        number = 10) </code></pre>
</div>
<div id="d---regularized-regression" class="section level3">
<h3>D - Regularized regression</h3>
<div id="standard-regression" class="section level4">
<h4>Standard regression</h4>
<ol style="list-style-type: decimal">
<li>Begin by fitting a standard regression model predicting <code>Grad.Rate</code> as a function of all other features. Specifically:</li>
</ol>
<ul>
<li>set the formula to <code>Grad.Rate ~ .</code>.</li>
<li>set the data to <code>college_train</code>.</li>
<li>set the method to <code>"glm"</code> for standard regression.</li>
<li>set the train control argument to <code>ctrl_cv</code>.</li>
</ul>
<pre class="r"><code># Standard regression 
graduation_glm &lt;- train(form = XX ~ .,
                        data = XX,
                        method = &quot;XX&quot;,
                        trControl = XX)</code></pre>
<pre class="r"><code># Standard regression 
graduation_glm &lt;- train(form = Grad.Rate ~ .,
                   data = college_train,
                   method = &quot;glm&quot;,
                   trControl = ctrl_cv)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>What were your final regression model coefficients?</li>
</ol>
<pre class="r"><code># Show final model
graduation_glm$finalModel</code></pre>
<pre><code>
Call:  NULL

Coefficients:
(Intercept)   PrivateYes         Apps       Accept       Enroll    Top10perc  
   2.24e+01     9.09e-01     7.30e-04    -6.45e-04     9.08e-03    -1.24e-01  
  Top25perc  F.Undergrad  P.Undergrad     Outstate   Room.Board        Books  
   3.76e-01    -1.54e-03    -1.31e-03     1.23e-03     1.59e-03     3.01e-04  
   Personal          PhD     Terminal    S.F.Ratio  perc.alumni       Expend  
  -2.20e-03     6.24e-03    -4.15e-02     4.28e-01     1.96e-01    -9.01e-05  

Degrees of Freedom: 499 Total (i.e. Null);  482 Residual
Null Deviance:      353000 
Residual Deviance: 277000   AIC: 4620</code></pre>
</div>
<div id="ridge-regression" class="section level4">
<h4>Ridge regression</h4>
<ol start="3" style="list-style-type: decimal">
<li>Before you can fit a regularized regression model like ridge regression, you need to determine a vector of lambda penalty values that the cross validation procedure will evaluate. Using the code below, create a vector called <code>lambda_vec</code> containing 100 values spanning a range from very close to <code>0</code> up to <code>100</code>.</li>
</ol>
<pre class="r"><code># Vector of lambda values to try
lambda_vec &lt;- 10 ^ (seq(-3, 2, length = 100))</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Using <code>train()</code>, fit a ridge regression model predicting <code>Grad.Rate</code> as a function of all features. This time make use of the <code>tuneGrid</code>, which will take a <code>data.frame</code> specifying the sets of tuning parameters to consider during cross validation. In addition to <code>alpha = 0</code>, which specifies a ridge penalty, add <code>lambda = lambda_vec</code>. Also, don’t forget to <code>"center"</code> and <code>"scale"</code> when using regularization.</li>
</ol>
<pre class="r"><code># Ridge regression 
graduation_ridge &lt;- train(form = XX ~ .,
                          data = XX,
                          method = &quot;XX&quot;,
                          trControl = XX,
                          preProcess = c(&quot;XX&quot;, &quot;XX&quot;),          # Standardize
                          tuneGrid = data.frame(alpha = 0,     # Ridge penalty
                                                lambda = XX))  # Penalty weight</code></pre>
<pre class="r"><code># Ridge regression
graduation_ridge &lt;- train(form = Grad.Rate ~ .,
                          data = college_train,
                          method = &quot;glmnet&quot;,
                          trControl = ctrl_cv,
                          preProcess = c(&quot;center&quot;, &quot;scale&quot;),    # Standardise
                          tuneGrid = data.frame(alpha = 0,      # Ridge penalty
                                                lambda = lambda_vec)) # Penalty weight</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Print your <code>graduation_ridge</code> object. Which lambda was selected as best performing?</li>
</ol>
<pre class="r"><code>graduation_ridge</code></pre>
<pre><code>glmnet 

500 samples
 17 predictor

Pre-processing: centered (17), scaled (17) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 449, 449, 451, 451, 450, 449, ... 
Resampling results across tuning parameters:

  lambda    RMSE  Rsquared  MAE 
  1.00e-03  24.3  0.176     19.5
  1.12e-03  24.3  0.176     19.5
  1.26e-03  24.3  0.176     19.5
  1.42e-03  24.3  0.176     19.5
  1.59e-03  24.3  0.176     19.5
  1.79e-03  24.3  0.176     19.5
  2.01e-03  24.3  0.176     19.5
  2.26e-03  24.3  0.176     19.5
  2.54e-03  24.3  0.176     19.5
  2.85e-03  24.3  0.176     19.5
  3.20e-03  24.3  0.176     19.5
  3.59e-03  24.3  0.176     19.5
  4.04e-03  24.3  0.176     19.5
  4.53e-03  24.3  0.176     19.5
  5.09e-03  24.3  0.176     19.5
  5.72e-03  24.3  0.176     19.5
  6.43e-03  24.3  0.176     19.5
  7.22e-03  24.3  0.176     19.5
  8.11e-03  24.3  0.176     19.5
  9.11e-03  24.3  0.176     19.5
  1.02e-02  24.3  0.176     19.5
  1.15e-02  24.3  0.176     19.5
  1.29e-02  24.3  0.176     19.5
  1.45e-02  24.3  0.176     19.5
  1.63e-02  24.3  0.176     19.5
  1.83e-02  24.3  0.176     19.5
  2.06e-02  24.3  0.176     19.5
  2.31e-02  24.3  0.176     19.5
  2.60e-02  24.3  0.176     19.5
  2.92e-02  24.3  0.176     19.5
  3.27e-02  24.3  0.176     19.5
  3.68e-02  24.3  0.176     19.5
  4.13e-02  24.3  0.176     19.5
  4.64e-02  24.3  0.176     19.5
  5.21e-02  24.3  0.176     19.5
  5.86e-02  24.3  0.176     19.5
  6.58e-02  24.3  0.176     19.5
  7.39e-02  24.3  0.176     19.5
  8.30e-02  24.3  0.176     19.5
  9.33e-02  24.3  0.176     19.5
  1.05e-01  24.3  0.176     19.5
  1.18e-01  24.3  0.176     19.5
  1.32e-01  24.3  0.176     19.5
  1.48e-01  24.3  0.176     19.5
  1.67e-01  24.3  0.176     19.5
  1.87e-01  24.3  0.176     19.5
  2.10e-01  24.3  0.176     19.5
  2.36e-01  24.3  0.176     19.5
  2.66e-01  24.3  0.176     19.5
  2.98e-01  24.3  0.176     19.5
  3.35e-01  24.3  0.176     19.5
  3.76e-01  24.3  0.176     19.5
  4.23e-01  24.3  0.176     19.5
  4.75e-01  24.3  0.176     19.5
  5.34e-01  24.3  0.176     19.5
  5.99e-01  24.3  0.176     19.5
  6.73e-01  24.3  0.176     19.5
  7.56e-01  24.3  0.176     19.5
  8.50e-01  24.3  0.176     19.5
  9.55e-01  24.3  0.176     19.5
  1.07e+00  24.3  0.176     19.5
  1.20e+00  24.3  0.177     19.5
  1.35e+00  24.3  0.177     19.5
  1.52e+00  24.3  0.178     19.5
  1.71e+00  24.3  0.178     19.5
  1.92e+00  24.3  0.179     19.5
  2.15e+00  24.3  0.180     19.5
  2.42e+00  24.3  0.180     19.5
  2.72e+00  24.3  0.181     19.5
  3.05e+00  24.2  0.182     19.4
  3.43e+00  24.2  0.183     19.4
  3.85e+00  24.2  0.184     19.4
  4.33e+00  24.2  0.184     19.4
  4.86e+00  24.2  0.185     19.4
  5.46e+00  24.2  0.186     19.4
  6.14e+00  24.2  0.187     19.4
  6.89e+00  24.2  0.188     19.4
  7.74e+00  24.1  0.188     19.4
  8.70e+00  24.1  0.189     19.4
  9.77e+00  24.1  0.190     19.4
  1.10e+01  24.1  0.190     19.4
  1.23e+01  24.1  0.191     19.4
  1.38e+01  24.1  0.192     19.4
  1.56e+01  24.1  0.192     19.3
  1.75e+01  24.1  0.193     19.3
  1.96e+01  24.1  0.193     19.3
  2.21e+01  24.1  0.193     19.4
  2.48e+01  24.1  0.194     19.4
  2.78e+01  24.2  0.194     19.4
  3.13e+01  24.2  0.194     19.4
  3.51e+01  24.2  0.194     19.4
  3.94e+01  24.2  0.194     19.4
  4.43e+01  24.2  0.194     19.4
  4.98e+01  24.3  0.194     19.5
  5.59e+01  24.3  0.194     19.5
  6.28e+01  24.4  0.194     19.5
  7.05e+01  24.4  0.193     19.6
  7.92e+01  24.5  0.193     19.6
  8.90e+01  24.5  0.193     19.7
  1.00e+02  24.6  0.193     19.7

Tuning parameter &#39;alpha&#39; was held constant at a value of 0
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0 and lambda = 15.6.</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Plot your <code>graduation_ridge</code> object. What do you see? Does this match the plot match the value identified in the previous task?</li>
</ol>
<pre class="r"><code># Plot graduation_ridge object
plot(XX)</code></pre>
<pre class="r"><code>plot(graduation_ridge)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-15-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="7" style="list-style-type: decimal">
<li>What were your final regression model coefficients for the best lambda value? Find them by running the following code.</li>
</ol>
<pre class="r"><code># Get coefficients from best lambda value
coef(graduation_ridge$finalModel, 
     graduation_ridge$bestTune$lambda)</code></pre>
<pre><code>18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                  1
(Intercept) 65.6024
PrivateYes   1.2076
Apps         0.7991
Accept       0.8294
Enroll       0.5064
Top10perc    1.5719
Top25perc    2.7798
F.Undergrad -0.3897
P.Undergrad -1.4911
Outstate     2.4885
Room.Board   1.5175
Books        0.0121
Personal    -1.3764
PhD          0.4341
Terminal     0.3793
S.F.Ratio    0.4892
perc.alumni  1.9803
Expend       0.4385</code></pre>
<ol start="8" style="list-style-type: decimal">
<li><p>How do these coefficients compare to what you found in regular regression? Are they similar? Could the differences have something to do with the applied scaling?</p></li>
<li><p>Using <code>predict()</code> save the fitted values of <code>graduation_glm</code> object as <code>glm_fit</code>.</p></li>
</ol>
<pre class="r"><code># Save fitted value
glm_fit &lt;- predict(graduation_glm)</code></pre>
</div>
<div id="lasso-regression" class="section level4">
<h4>Lasso regression</h4>
<ol start="10" style="list-style-type: decimal">
<li>Now fit a lasso regression model predicting <code>Grad.Rate</code> as a function of all features. Set <code>alpha = 1</code> for the Lasso penalty and add <code>lambda = lambda_vec</code> as above.</li>
</ol>
<pre class="r"><code># Lasso regression 
graduation_lasso &lt;- train(form = XX ~ .,
                          data = XX,
                          method = &quot;XX&quot;,
                          trControl = XX,
                          preProcess = c(&quot;XX&quot;, &quot;XX&quot;),         # Standardise
                          tuneGrid = data.frame(alpha = XX,   # Lasso penalty
                                                lambda = XX)) # Penalty weight</code></pre>
<pre class="r"><code># Lasso regression 
graduation_lasso &lt;- train(form = Grad.Rate ~ .,
                          data = college_train,
                          method = &quot;glmnet&quot;,
                          trControl = ctrl_cv,
                          preProcess = c(&quot;center&quot;, &quot;scale&quot;),   # Standardise
                          tuneGrid = data.frame(alpha = 1,     # Lasso penalty
                                                 lambda = lambda_vec)) # Penalty weight</code></pre>
<ol start="11" style="list-style-type: decimal">
<li>Print your <code>graduation_lasso</code> object. Which lambda was selected as best performing?</li>
</ol>
<pre class="r"><code>graduation_lasso</code></pre>
<pre><code>glmnet 

500 samples
 17 predictor

Pre-processing: centered (17), scaled (17) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 448, 449, 451, 448, 450, 451, ... 
Resampling results across tuning parameters:

  lambda    RMSE  Rsquared  MAE 
  1.00e-03  24.5  0.1656    19.6
  1.12e-03  24.5  0.1656    19.6
  1.26e-03  24.5  0.1656    19.6
  1.42e-03  24.5  0.1656    19.6
  1.59e-03  24.5  0.1656    19.6
  1.79e-03  24.5  0.1656    19.6
  2.01e-03  24.5  0.1656    19.6
  2.26e-03  24.5  0.1656    19.6
  2.54e-03  24.5  0.1656    19.6
  2.85e-03  24.5  0.1656    19.6
  3.20e-03  24.5  0.1656    19.6
  3.59e-03  24.5  0.1656    19.6
  4.04e-03  24.5  0.1656    19.6
  4.53e-03  24.5  0.1656    19.6
  5.09e-03  24.5  0.1656    19.6
  5.72e-03  24.5  0.1656    19.6
  6.43e-03  24.5  0.1657    19.6
  7.22e-03  24.5  0.1657    19.6
  8.11e-03  24.5  0.1658    19.6
  9.11e-03  24.5  0.1659    19.6
  1.02e-02  24.5  0.1659    19.6
  1.15e-02  24.5  0.1660    19.6
  1.29e-02  24.5  0.1661    19.6
  1.45e-02  24.5  0.1662    19.6
  1.63e-02  24.5  0.1664    19.6
  1.83e-02  24.5  0.1665    19.6
  2.06e-02  24.5  0.1667    19.6
  2.31e-02  24.5  0.1669    19.6
  2.60e-02  24.5  0.1671    19.6
  2.92e-02  24.5  0.1674    19.6
  3.27e-02  24.5  0.1676    19.6
  3.68e-02  24.4  0.1679    19.6
  4.13e-02  24.4  0.1682    19.6
  4.64e-02  24.4  0.1686    19.6
  5.21e-02  24.4  0.1689    19.6
  5.86e-02  24.4  0.1693    19.6
  6.58e-02  24.4  0.1696    19.6
  7.39e-02  24.4  0.1700    19.6
  8.30e-02  24.4  0.1704    19.6
  9.33e-02  24.4  0.1710    19.6
  1.05e-01  24.4  0.1717    19.6
  1.18e-01  24.4  0.1724    19.6
  1.32e-01  24.3  0.1732    19.6
  1.48e-01  24.3  0.1741    19.6
  1.67e-01  24.3  0.1751    19.5
  1.87e-01  24.3  0.1761    19.5
  2.10e-01  24.3  0.1766    19.5
  2.36e-01  24.3  0.1774    19.5
  2.66e-01  24.2  0.1785    19.5
  2.98e-01  24.2  0.1795    19.5
  3.35e-01  24.2  0.1804    19.5
  3.76e-01  24.2  0.1813    19.5
  4.23e-01  24.2  0.1822    19.5
  4.75e-01  24.2  0.1828    19.5
  5.34e-01  24.2  0.1833    19.5
  5.99e-01  24.2  0.1837    19.5
  6.73e-01  24.2  0.1842    19.5
  7.56e-01  24.2  0.1846    19.5
  8.50e-01  24.2  0.1851    19.5
  9.55e-01  24.2  0.1856    19.5
  1.07e+00  24.2  0.1860    19.5
  1.20e+00  24.2  0.1861    19.5
  1.35e+00  24.2  0.1863    19.5
  1.52e+00  24.2  0.1867    19.5
  1.71e+00  24.2  0.1865    19.6
  1.92e+00  24.2  0.1859    19.6
  2.15e+00  24.3  0.1856    19.6
  2.42e+00  24.3  0.1856    19.7
  2.72e+00  24.3  0.1861    19.7
  3.05e+00  24.4  0.1868    19.7
  3.43e+00  24.4  0.1880    19.7
  3.85e+00  24.5  0.1887    19.8
  4.33e+00  24.6  0.1892    19.9
  4.86e+00  24.7  0.1896    20.0
  5.46e+00  24.9  0.1894    20.1
  6.14e+00  25.1  0.1883    20.3
  6.89e+00  25.4  0.1856    20.5
  7.74e+00  25.7  0.1798    20.8
  8.70e+00  26.1  0.1625    21.1
  9.77e+00  26.5  0.0878    21.4
  1.10e+01  26.5     NaN    21.5
  1.23e+01  26.5     NaN    21.5
  1.38e+01  26.5     NaN    21.5
  1.56e+01  26.5     NaN    21.5
  1.75e+01  26.5     NaN    21.5
  1.96e+01  26.5     NaN    21.5
  2.21e+01  26.5     NaN    21.5
  2.48e+01  26.5     NaN    21.5
  2.78e+01  26.5     NaN    21.5
  3.13e+01  26.5     NaN    21.5
  3.51e+01  26.5     NaN    21.5
  3.94e+01  26.5     NaN    21.5
  4.43e+01  26.5     NaN    21.5
  4.98e+01  26.5     NaN    21.5
  5.59e+01  26.5     NaN    21.5
  6.28e+01  26.5     NaN    21.5
  7.05e+01  26.5     NaN    21.5
  7.92e+01  26.5     NaN    21.5
  8.90e+01  26.5     NaN    21.5
  1.00e+02  26.5     NaN    21.5

Tuning parameter &#39;alpha&#39; was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 1 and lambda = 0.955.</code></pre>
<ol start="12" style="list-style-type: decimal">
<li>Plot your <code>graduation_lasso</code> object. What do you see? Does this match the plot match the value identified in the previous task?</li>
</ol>
<pre class="r"><code># Plot model object
plot(XX)</code></pre>
<pre class="r"><code>plot(graduation_lasso)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-22-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="13" style="list-style-type: decimal">
<li>What were your final regression model coefficients for the best lambda value? Find them by running the following code.</li>
</ol>
<pre class="r"><code># Get coefficients from best lambda value
coef(graduation_lasso$finalModel, 
     graduation_lasso$bestTune$lambda)</code></pre>
<pre><code>18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                 1
(Intercept) 65.602
PrivateYes   .    
Apps         .    
Accept       0.510
Enroll       .    
Top10perc    .    
Top25perc    5.363
F.Undergrad  .    
P.Undergrad -0.938
Outstate     4.430
Room.Board   1.050
Books        .    
Personal    -1.080
PhD          .    
Terminal     .    
S.F.Ratio    .    
perc.alumni  1.646
Expend       .    </code></pre>
<ol start="14" style="list-style-type: decimal">
<li>How do these coefficients compare to what you found for the regular and ridge regression? Have some features been set to 0?</li>
</ol>
</div>
<div id="evaluate-performance" class="section level4">
<h4>Evaluate performance</h4>
<ol start="15" style="list-style-type: decimal">
<li>Store the training data and test data criterion (<code>Grad.Rate</code>) as <code>criterion_train</code> and <code>criterion_test</code>.</li>
</ol>
<pre class="r"><code># store criteria
criterion_train &lt;- college_train$Grad.Rate
criterion_test &lt;- college_test$Grad.Rate</code></pre>
<p>s 16. Using <code>predict()</code>, save the fitted values of your models as <code>glm_fit</code>, <code>ridge_fit</code>, and <code>lasso_fit</code>.</p>
<pre class="r"><code># store fitted values
glm_fit &lt;- predict(graduation_glm)
ridge_fit &lt;- predict(graduation_ridge)
lasso_fit &lt;- predict(graduation_lasso)</code></pre>
<ol start="17" style="list-style-type: decimal">
<li>Using <code>postResample</code> evaluate the fitting performance of your models. Which model has the best performance in fitting the training data?</li>
</ol>
<pre class="r"><code># evaluate fit
postResample(pred = glm_fit, obs = criterion_train)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  23.533    0.215   18.886 </code></pre>
<pre class="r"><code>postResample(pred = ridge_fit, obs = criterion_train)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  23.791    0.202   19.073 </code></pre>
<pre class="r"><code>postResample(pred = lasso_fit, obs = criterion_train)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  23.773    0.201   19.131 </code></pre>
<ol start="18" style="list-style-type: decimal">
<li>Using <code>predict()</code> and <code>newdata = college_test</code>, save the predicted values of your models as <code>glm_pred</code>, <code>ridge_pred</code>, and <code>lasso_pred</code>.</li>
</ol>
<pre class="r"><code># store predicted values
glm_pred &lt;- predict(graduation_glm, newdata = college_test)
ridge_pred &lt;- predict(graduation_ridge, newdata = college_test)
lasso_pred &lt;- predict(graduation_lasso, newdata = college_test)</code></pre>
<ol start="19" style="list-style-type: decimal">
<li>Using <code>postResample</code> evaluate the prediction performance of your models. Which model has the best performance in predicting the test data? Did the regularized regressions outperform the unregularized one?</li>
</ol>
<pre class="r"><code># evaluate fit
postResample(pred = glm_pred, obs = criterion_test)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  13.762    0.405    9.985 </code></pre>
<pre class="r"><code>postResample(pred = ridge_pred, obs = criterion_test)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  13.244    0.451    9.804 </code></pre>
<pre class="r"><code>postResample(pred = lasso_pred, obs = criterion_test)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  13.598    0.415   10.106 </code></pre>
</div>
</div>
<div id="e---trees" class="section level3">
<h3>E - Trees</h3>
<div id="decision-tree" class="section level4">
<h4>Decision tree</h4>
<ol style="list-style-type: decimal">
<li>It’s time to see what parameter tuning can do for decision trees and random forests. To do this, first, determine a vector of possible values for the complexity parameter <code>cp</code> of decision trees. To this end, using the code below, create a vector called <code>cp_vec</code> which contains 100 values between 0 and .2.</li>
</ol>
<pre class="r"><code># Determine possible values for cp
cp_vec &lt;- seq(from = 0, to = .2, length = 100)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Using <code>train()</code>, fit a decision tree model called <code>graduation_rpart</code> predicting <code>Grad.Rate</code>by all features. Again, assign a data frame to <code>tuneGrid</code> specifying the possible tuning parameters, i.e., <code>cp = cp_vec</code>.</li>
</ol>
<pre class="r"><code># Decision tree
graduation_rpart &lt;- train(form = Grad.part ~ .,
                          data = XX,
                          method = &quot;XX&quot;,
                          trControl = XX,
                          tuneGrid = data.frame(cp = XX))</code></pre>
<pre class="r"><code># Decision tree
graduation_rpart &lt;- train(form = Grad.Rate ~ .,
                  data = college_train,
                  method = &quot;rpart&quot;,
                  trControl = ctrl_cv,
                  tuneGrid = data.frame(cp = cp_vec))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Print your <code>graduation_rpart</code> object. Which <code>cp</code> was selected as best performing?</li>
</ol>
<pre class="r"><code>graduation_rpart</code></pre>
<pre><code>CART 

500 samples
 17 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 449, 450, 451, 449, 450, 451, ... 
Resampling results across tuning parameters:

  cp       RMSE  Rsquared  MAE 
  0.00000  28.6  0.08318   22.9
  0.00202  28.6  0.08335   22.9
  0.00404  28.6  0.08270   22.9
  0.00606  28.4  0.08190   22.9
  0.00808  28.1  0.08318   22.4
  0.01010  27.3  0.09422   21.9
  0.01212  26.5  0.10928   21.4
  0.01414  26.4  0.11320   21.2
  0.01616  26.5  0.10038   21.6
  0.01818  26.3  0.10091   21.3
  0.02020  26.0  0.10572   21.3
  0.02222  26.1  0.09805   21.3
  0.02424  26.2  0.08947   21.3
  0.02626  26.1  0.08935   21.2
  0.02828  26.1  0.08935   21.2
  0.03030  26.1  0.08825   21.2
  0.03232  26.1  0.08554   21.2
  0.03434  26.1  0.08554   21.2
  0.03636  26.4  0.06852   21.3
  0.03838  26.4  0.07288   21.3
  0.04040  26.6  0.06431   21.6
  0.04242  26.4  0.06471   21.5
  0.04444  26.5  0.05820   21.6
  0.04646  26.5  0.05820   21.6
  0.04848  26.5  0.05691   21.6
  0.05051  26.5  0.05691   21.6
  0.05253  26.5  0.05691   21.6
  0.05455  26.5  0.05691   21.6
  0.05657  26.5  0.05691   21.6
  0.05859  26.5  0.05691   21.6
  0.06061  26.5  0.05691   21.6
  0.06263  26.5  0.05691   21.6
  0.06465  26.5  0.05691   21.6
  0.06667  26.5  0.05691   21.6
  0.06869  26.5  0.05691   21.6
  0.07071  26.5  0.05691   21.6
  0.07273  26.5  0.05691   21.6
  0.07475  26.5  0.05691   21.6
  0.07677  26.5  0.05691   21.6
  0.07879  26.5  0.05691   21.6
  0.08081  26.5  0.05691   21.6
  0.08283  26.5  0.05691   21.6
  0.08485  26.5  0.05691   21.6
  0.08687  26.5  0.05691   21.6
  0.08889  26.5  0.05691   21.6
  0.09091  26.5  0.05691   21.6
  0.09293  26.5  0.05691   21.6
  0.09495  26.5  0.05691   21.6
  0.09697  26.5  0.05691   21.6
  0.09899  26.5  0.05691   21.6
  0.10101  26.5  0.05691   21.6
  0.10303  26.5  0.05691   21.6
  0.10505  26.9  0.03264   21.9
  0.10707  26.9  0.03264   21.9
  0.10909  27.1  0.01439   22.1
  0.11111  27.1  0.01439   22.1
  0.11313  27.1  0.01439   22.1
  0.11515  27.1  0.01439   22.1
  0.11717  27.1  0.00787   22.1
  0.11919  27.1  0.00787   22.1
  0.12121  27.1  0.00564   22.0
  0.12323  27.1  0.00564   22.0
  0.12525  27.0  0.00530   21.9
  0.12727  27.0  0.00530   21.9
  0.12929  26.9  0.00567   21.8
  0.13131  26.8  0.00577   21.7
  0.13333  26.5      NaN   21.5
  0.13535  26.5      NaN   21.5
  0.13737  26.5      NaN   21.5
  0.13939  26.5      NaN   21.5
  0.14141  26.5      NaN   21.5
  0.14343  26.5      NaN   21.5
  0.14545  26.5      NaN   21.5
  0.14747  26.5      NaN   21.5
  0.14949  26.5      NaN   21.5
  0.15152  26.5      NaN   21.5
  0.15354  26.5      NaN   21.5
  0.15556  26.5      NaN   21.5
  0.15758  26.5      NaN   21.5
  0.15960  26.5      NaN   21.5
  0.16162  26.5      NaN   21.5
  0.16364  26.5      NaN   21.5
  0.16566  26.5      NaN   21.5
  0.16768  26.5      NaN   21.5
  0.16970  26.5      NaN   21.5
  0.17172  26.5      NaN   21.5
  0.17374  26.5      NaN   21.5
  0.17576  26.5      NaN   21.5
  0.17778  26.5      NaN   21.5
  0.17980  26.5      NaN   21.5
  0.18182  26.5      NaN   21.5
  0.18384  26.5      NaN   21.5
  0.18586  26.5      NaN   21.5
  0.18788  26.5      NaN   21.5
  0.18990  26.5      NaN   21.5
  0.19192  26.5      NaN   21.5
  0.19394  26.5      NaN   21.5
  0.19596  26.5      NaN   21.5
  0.19798  26.5      NaN   21.5
  0.20000  26.5      NaN   21.5

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.0202.</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Plot your <code>graduation_rpart</code> object. What do you see? Does this match the plot match the value identified in the previous task?</li>
</ol>
<pre class="r"><code>plot(graduation_rpart)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-33-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Plot your final decision tree using the following code. Do you find the model sensible?</li>
</ol>
<pre class="r"><code># Visualise your trees
plot(as.party(graduation_rpart$finalModel)) </code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-34-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="6" style="list-style-type: decimal">
<li>How do the nodes in the tree compare to those in the ridge or lasso models?</li>
</ol>
</div>
<div id="random-forest" class="section level4">
<h4>Random forest</h4>
<ol start="7" style="list-style-type: decimal">
<li>Now onto fitting a random forest. Using the code below, create a vector called <code>mtry_vec</code> containing values from 1 to 5, the tuning parameter candidates for our random forest.</li>
</ol>
<pre class="r"><code># mtry candidates
mtry_vec &lt;- 1:5</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Fit a random forest model predicting <code>Grad.Rate</code> as a function of all features. Make sure to use <code>mtry = mtry_vec</code> within the data frame specifying the <code>tuneGrid</code>. This one might take a bit longer than usual.</li>
</ol>
<pre class="r"><code># Random forest
graduation_rf &lt;- train(form = XX ~ .,
                   data = XX,
                   method = &quot;XX&quot;,
                   trControl = XX,
                   tuneGrid = data.frame(mtry = XX))</code></pre>
<pre class="r"><code># Random forest
graduation_rf &lt;- train(form = Grad.Rate ~ .,
                   data = college_train,
                   method = &quot;rf&quot;,
                   trControl = ctrl_cv,
                   tuneGrid = data.frame(mtry = mtry_vec))</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>Print your <code>graduation_rf</code> object. What do you see? Which <code>mtry</code> was selected as best performing?</li>
</ol>
<pre class="r"><code>graduation_rf</code></pre>
<pre><code>Random Forest 

500 samples
 17 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 449, 449, 450, 451, 450, 449, ... 
Resampling results across tuning parameters:

  mtry  RMSE  Rsquared  MAE 
  1     24.4  0.158     19.7
  2     24.5  0.155     19.7
  3     24.7  0.144     19.9
  4     24.6  0.148     19.9
  5     24.6  0.152     19.8

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 1.</code></pre>
<ol start="10" style="list-style-type: decimal">
<li>Plot your <code>graduation_rf</code> object. What do you see? Does this match the plot match the value identified in the previous task?</li>
</ol>
<pre class="r"><code>plot(graduation_rf)</code></pre>
<p><img src="Optimization_practical_files/figure-html/unnamed-chunk-39-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="evaluate-performance-1" class="section level4">
<h4>Evaluate performance</h4>
<ol start="11" style="list-style-type: decimal">
<li>Using <code>predict()</code>, save the fitted values of your tree models as <code>rpart_fit</code> and <code>rf_fit</code>.</li>
</ol>
<pre class="r"><code># store fitted values
rpart_fit &lt;- predict(graduation_rpart)
rf_fit &lt;- predict(graduation_rf)</code></pre>
<ol start="12" style="list-style-type: decimal">
<li>Using <code>postResample</code> evaluate the fitting performance of your models. Which model has the best performance in fitting the training data? If you like compare to the regression models of the previous section.</li>
</ol>
<pre class="r"><code># evaluate fit
postResample(pred = rpart_fit, obs = criterion_train)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  24.039    0.181   19.511 </code></pre>
<pre class="r"><code>postResample(pred = rf_fit, obs = criterion_train)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  13.104    0.884   10.503 </code></pre>
<ol start="13" style="list-style-type: decimal">
<li>Using <code>predict()</code> and <code>newdata = college_test</code>, save the predicted values of your models as <code>rpart_pred</code>, and <code>rf_pred</code>.</li>
</ol>
<pre class="r"><code># store predicted values
rpart_pred &lt;- predict(graduation_rpart, newdata = college_test)
rf_pred &lt;- predict(graduation_rf, newdata = college_test)</code></pre>
<ol start="14" style="list-style-type: decimal">
<li>Using <code>postResample</code> evaluate the prediction performance of your models. Which model has the best performance in predicting the test data? Did the tree models outperform the regularized regressions?</li>
</ol>
<pre class="r"><code># evaluate fit
postResample(pred = rpart_pred, obs = criterion_test)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  16.299    0.205   12.495 </code></pre>
<pre class="r"><code>postResample(pred = rf_pred, obs = criterion_test)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
   13.66     0.41    10.06 </code></pre>
</div>
</div>
<div id="x---challenges-explore-tuning-parameter-grids" class="section level3">
<h3>X - Challenges: Explore tuning parameter grids</h3>
<ol style="list-style-type: decimal">
<li>The name <code>tuneGrid</code> already suggests that one may want to vary multiple tuning parameters at the same time. A handy function helping in this is <code>expand.grid()</code>, which will produce all compbinations of values of the vectors supplied as its arguments. Try, e.g., <code>expand.grid(a = c(1, 2), b = c(2, 3, 4))</code>. The template below shows you how you can use <code>expand.grid()</code> to specify multiple tuning parameters at the same time.</li>
</ol>
<pre class="r"><code>model &lt;- train(form = XX ~ .,
               data = XX,
               method = &quot;XX&quot;,
               trControl = XX,
               preProcess = c(&quot;XX&quot;, &quot;XX&quot;),         
               tuneGrid = expand.grid(parameter_1 = XX,    
                                      parameter_2 = XX)) </code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>Run and evaluate a regularized regression that uses cross validation to not only identify the best value for <code>lambda</code> but also the best value for <code>alpha</code>, e.g., using <code>alpha = c(0, .5, 1)</code>. This way you can let the procedure decide whether to use ridge, lasso or both.</p></li>
<li><p>Run and evaluate a random forest while tuning not only <code>mtry</code> but also <code>ntree</code>, e.g., using <code>ntree = c(100,500,1000)</code>. Tip: avoid high values for <code>ntree</code> or <code>mtry</code>.</p></li>
<li><p>As done in the previous sessions try predicting <code>Private</code> rather than <code>Grad.Rate</code>. Note, this may require a different range of lambda values. You’ll figure it out.</p></li>
</ol>
</div>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<pre class="r"><code># Model optimization with Regression

# Step 0: Load packages-----------
library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 
library(partykit)     # For decision trees
library(party)        # For decision trees

# Step 1: Load, clean, and explore data ----------------------

# training data
data_train &lt;- read_csv(&quot;1_Data/diamonds_train.csv&quot;)

# test data
data_test &lt;- read_csv(&quot;1_Data/diamonds_test.csv&quot;)

# Convert all characters to factor
#  Some ML models require factors
data_train &lt;- data_train %&gt;%
  mutate_if(is.character, factor)

data_test &lt;- data_test %&gt;%
  mutate_if(is.character, factor)

# Explore training data
data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Define criterion_train
criterion_train &lt;- data_train$price
criterion_test &lt;- data_test$price

# Step 2: Define training control parameters -------------

# Use 10-fold cross validation
ctrl_cv &lt;- trainControl(method = &quot;cv&quot;, 
                        number = 10) 

# Step 3: Train models: -----------------------------

# Normal Regression --------------------------
price_glm &lt;- train(form = price ~ carat + depth + table + x + y,
                   data = data_train,
                   method = &quot;glm&quot;,
                   trControl = ctrl_cv)


# Print key results
price_glm

# Coefficients
coef(price_glm$finalModel)

# Lasso --------------------------

# Vector of lambda values to try
lambda_vec &lt;- 10 ^ seq(-3, 3, length = 100)

price_lasso &lt;- train(form = price ~ carat + depth + table + x + y,
                   data = data_train,
                   method = &quot;glmnet&quot;,
                   trControl = ctrl_cv,
                   preProcess = c(&quot;center&quot;, &quot;scale&quot;),  # Standardise
                   tuneGrid = data.frame(alpha = 1,  # Lasso
                                          lambda = lambda_vec))


# Print key results
price_lasso

# Plot regularisation parameter versus error
plot(price_lasso)

# Print best regularisation parameter
price_lasso$bestTune$lambda

# Get coefficients from best lambda value
coef(price_lasso$finalModel, 
     price_lasso$bestTune$lambda)

# Ridge --------------------------

# Vector of lambda values to try
lambda_vec &lt;- 10 ^ seq(-3, 3, length = 100)

price_ridge &lt;- train(form = price ~ carat + depth + table + x + y,
                     data = data_train,
                     method = &quot;glmnet&quot;,
                     trControl = ctrl_cv,
                     preProcess = c(&quot;center&quot;, &quot;scale&quot;),  # Standardise
                     tuneGrid = data.frame(alpha = 0,     # Ridge penalty
                                            lambda = lambda_vec))

# Print key results
price_ridge

# Plot regularisation parameter versus error
plot(price_ridge)

# Print best regularisation parameter
price_ridge$bestTune$lambda

# Get coefficients from best lambda value
coef(price_ridge$finalModel, 
     price_ridge$bestTune$lambda)


# Decision Trees --------------------------

# Vector of cp values to try
cp_vec &lt;- seq(0, .1, length = 100)

price_rpart &lt;- train(form = price ~ carat + depth + table + x + y,
                  data = data_train,
                  method = &quot;rpart&quot;,
                  trControl = ctrl_cv,
                  tuneGrid = data.frame(cp = cp_vec))

# Print key results
price_rpart

# Plot complexity parameter vs. error
plot(price_rpart)

# Print best complexity parameter
price_rpart$bestTune$cp</code></pre>
</div>
<div id="datasets" class="section level2">
<h2>Datasets</h2>
<table>
<thead>
<tr class="header">
<th align="left">File</th>
<th align="left">Rows</th>
<th align="left">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_train.csv">college_train.csv</a></td>
<td align="left">50</td>
<td align="left">20</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv">college_test.csv</a></td>
<td align="left">213</td>
<td align="left">20</td>
</tr>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_train.csv">college_train.csv</a></td>
<td align="left">500</td>
<td align="left">18</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv">college_test.csv</a></td>
<td align="left">277</td>
<td align="left">18</td>
</tr>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/house_train.csv">house_train.csv</a></td>
<td align="left">5000</td>
<td align="left">21</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv">house_test.csv</a></td>
<td align="left">1000</td>
<td align="left">21</td>
</tr>
</tbody>
</table>
<ul>
<li><p>The <code>college_train</code> and <code>college_test</code> data are taken from the <code>College</code> dataset in the <code>ISLR</code> package. They contain statistics for a large number of US Colleges from the 1995 issue of US News and World Report.</p></li>
<li><p>The <code>house_train</code> and <code>house_test</code> data come from <a href="https://www.kaggle.com/harlfoxem/housesalesprediction">https://www.kaggle.com/harlfoxem/housesalesprediction</a></p></li>
</ul>
<div id="variable-description-of-college_train-and-college_test" class="section level4">
<h4>Variable description of <code>college_train</code> and <code>college_test</code></h4>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>Private</code></td>
<td align="left">A factor with levels No and Yes indicating private or public university.</td>
</tr>
<tr class="even">
<td align="left"><code>Apps</code></td>
<td align="left">Number of applications received.</td>
</tr>
<tr class="odd">
<td align="left"><code>Accept</code></td>
<td align="left">Number of applications accepted.</td>
</tr>
<tr class="even">
<td align="left"><code>Enroll</code></td>
<td align="left">Number of new students enrolled.</td>
</tr>
<tr class="odd">
<td align="left"><code>Top10perc</code></td>
<td align="left">Pct. new students from top 10% of H.S. class.</td>
</tr>
<tr class="even">
<td align="left"><code>Top25perc</code></td>
<td align="left">Pct. new students from top 25% of H.S. class.</td>
</tr>
<tr class="odd">
<td align="left"><code>F.Undergrad</code></td>
<td align="left">Number of fulltime undergraduates.</td>
</tr>
<tr class="even">
<td align="left"><code>P.Undergrad</code></td>
<td align="left">Number of parttime undergraduates.</td>
</tr>
<tr class="odd">
<td align="left"><code>Outstate</code></td>
<td align="left">Out-of-state tuition.</td>
</tr>
<tr class="even">
<td align="left"><code>Room.Board</code></td>
<td align="left">Room and board costs.</td>
</tr>
<tr class="odd">
<td align="left"><code>Books</code></td>
<td align="left">Estimated book costs.</td>
</tr>
<tr class="even">
<td align="left"><code>Personal</code></td>
<td align="left">Estimated personal spending.</td>
</tr>
<tr class="odd">
<td align="left"><code>PhD</code></td>
<td align="left">Pct. of faculty with Ph.D.’s.</td>
</tr>
<tr class="even">
<td align="left"><code>Terminal</code></td>
<td align="left">Pct. of faculty with terminal degree.</td>
</tr>
<tr class="odd">
<td align="left"><code>S.F.Ratio</code></td>
<td align="left">Student/faculty ratio.</td>
</tr>
<tr class="even">
<td align="left"><code>perc.alumni</code></td>
<td align="left">Pct. alumni who donate.</td>
</tr>
<tr class="odd">
<td align="left"><code>Expend</code></td>
<td align="left">Instructional expenditure per student.</td>
</tr>
<tr class="even">
<td align="left"><code>Grad.Rate</code></td>
<td align="left">Graduation rate.</td>
</tr>
</tbody>
</table>
</div>
<div id="variable-description-of-house_train-and-house_test" class="section level4">
<h4>Variable description of <code>house_train</code> and <code>house_test</code></h4>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>price</code></td>
<td align="left">Price of the house in $.</td>
</tr>
<tr class="even">
<td align="left"><code>bedrooms</code></td>
<td align="left">Number of bedrooms.</td>
</tr>
<tr class="odd">
<td align="left"><code>bathrooms</code></td>
<td align="left">Number of bathrooms.</td>
</tr>
<tr class="even">
<td align="left"><code>sqft_living</code></td>
<td align="left">Square footage of the home.</td>
</tr>
<tr class="odd">
<td align="left"><code>sqft_lot</code></td>
<td align="left">Square footage of the lot.</td>
</tr>
<tr class="even">
<td align="left"><code>floors</code></td>
<td align="left">Total floors (levels) in house.</td>
</tr>
<tr class="odd">
<td align="left"><code>waterfront</code></td>
<td align="left">House which has a view to a waterfront.</td>
</tr>
<tr class="even">
<td align="left"><code>view</code></td>
<td align="left">Has been viewed.</td>
</tr>
<tr class="odd">
<td align="left"><code>condition</code></td>
<td align="left">How good the condition is (Overall).</td>
</tr>
<tr class="even">
<td align="left"><code>grade</code></td>
<td align="left">Overall grade given to the housing unit, based on King County grading system.</td>
</tr>
<tr class="odd">
<td align="left"><code>sqft_above</code></td>
<td align="left">Square footage of house apart from basement.</td>
</tr>
<tr class="even">
<td align="left"><code>sqft_basement</code></td>
<td align="left">Square footage of the basement.</td>
</tr>
<tr class="odd">
<td align="left"><code>yr_built</code></td>
<td align="left">Built Year.</td>
</tr>
<tr class="even">
<td align="left"><code>yr_renovated</code></td>
<td align="left">Year when house was renovated.</td>
</tr>
<tr class="odd">
<td align="left"><code>zipcode</code></td>
<td align="left">Zip code.</td>
</tr>
<tr class="even">
<td align="left"><code>lat</code></td>
<td align="left">Latitude coordinate.</td>
</tr>
<tr class="odd">
<td align="left"><code>long</code></td>
<td align="left">Longitude coordinate.</td>
</tr>
<tr class="even">
<td align="left"><code>sqft_living15</code></td>
<td align="left">Living room area in 2015 (implies some renovations). This might or might not have affected the lotsize area.</td>
</tr>
<tr class="odd">
<td align="left"><code>sqft_lot15</code></td>
<td align="left">lot-size area in 2015 (implies some renovations).</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="functions" class="section level2">
<h2>Functions</h2>
<div id="packages" class="section level3">
<h3>Packages</h3>
<table>
<thead>
<tr class="header">
<th align="left">Package</th>
<th align="left">Installation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>tidyverse</code></td>
<td align="left"><code>install.packages("tidyverse")</code></td>
</tr>
<tr class="even">
<td align="left"><code>caret</code></td>
<td align="left"><code>install.packages("caret")</code></td>
</tr>
<tr class="odd">
<td align="left"><code>partykit</code></td>
<td align="left"><code>install.packages("partykit")</code></td>
</tr>
<tr class="even">
<td align="left"><code>party</code></td>
<td align="left"><code>install.packages("party")</code></td>
</tr>
</tbody>
</table>
</div>
<div id="functions-1" class="section level3">
<h3>Functions</h3>
<table>
<colgroup>
<col width="7%" />
<col width="12%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Function</th>
<th align="left">Package</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>trainControl()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Define modelling control parameters</td>
</tr>
<tr class="even">
<td align="left"><code>train()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Train a model</td>
</tr>
<tr class="odd">
<td align="left"><code>predict(object, newdata)</code></td>
<td align="left"><code>stats</code></td>
<td align="left">Predict the criterion values of <code>newdata</code> based on <code>object</code></td>
</tr>
<tr class="even">
<td align="left"><code>postResample()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in regression tasks</td>
</tr>
<tr class="odd">
<td align="left"><code>confusionMatrix()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in classification tasks</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="resources" class="section level2">
<h2>Resources</h2>
<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf"> <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="Trulli" style="width:70%"></a><br> <font style="font-size:10px"> from <a href= "https://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption">github.com/rstudio</a></font>
</figure>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
