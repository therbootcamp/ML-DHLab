---
title: "Optimization"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Machine Learning with R</font><br>
      <a href='https://therbootcamp.github.io/ML-DHLab/'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>The R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = TRUE, 
                      eval = TRUE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)

set.seed(100)
```

```{r, message = FALSE, eval = TRUE, echo = FALSE}
# Load datasets locally
library(tidyverse)
college_train <- read_csv("1_Data/college_train.csv")
college_test <- read_csv("1_Data/college_test.csv")
house_train <- read_csv("1_Data/house_train.csv")
house_test <- read_csv("1_Data/house_test.csv")
```
<p align="center">
<img width="100%" src="image/rexthor.png" margin=0><br>
<font style="font-size:10px">from [xkcd.com](https://xkcd.com/1725/)</font>
</p>

# {.tabset}

## Overview

By the end of this practical you will know how to:

1. Use cross-validation to select optimal model tuning parameters for decision trees and random forests.
2. Compare 'standard' regression with lasso and ridge penalised regression.

## Tasks


### A - Setup

1. Open your `TheRBootcamp` R project. 

2. Open a new R script. At the top of the script, using comments, write your name and the date. 

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Optimizing practical
```

3. Save the script as a new file called `Optimization_practical.R` in the `2_Code` folder.  

4. Using `library()` load the packages `tidyverse`, `caret`, `party`, `partykit`.

```{r, eval = TRUE}
# Load packages necessary for this script
library(tidyverse)
library(caret)
library(party)
library(partykit)
```

### B - Load the `graduation` data

1. You will again begin by analyzing the graduation data. Read in the data sets `graduation_train.csv` and `graduation_test.csv` and convert all character to factors. 

```{r, echo = TRUE}
# Read college data
college_train <- read_csv(file = "1_Data/college_train.csv")
college_test <- read_csv(file = "1_Data/college_test.csv")

# Convert all character features to factor
college_train <- college_train %>%
  mutate_if(is.character, factor) 
college_test <- college_test %>%
          mutate_if(is.character, factor)
```

### C - Setup `trainControl`

1. Now, you finally make use of the train control object by specifying 10-fold cross-validation as the preferred optimization method in an object called `ctrl_cv`. Specifically:

- set `method = "cv"` to specify cross validation.
- set `number = 10` to specify 10 folds.

```{r, eval = FALSE, echo = TRUE}
# Use 10-fold cross validation
ctrl_cv <- trainControl(method = "XX", 
                        number = XX) 
```

```{r}
# Use 10-fold cross validation
ctrl_cv <- trainControl(method = "cv", 
                        number = 10) 
```

### D - Regularized regression

#### Standard regression

1. Begin by fitting a standard regression model predicting `Grad.Rate` as a function of all other features. Specifically:

- set the formula to `Grad.Rate ~ .`.
- set the data to `college_train`.
- set the method to `"glm"` for standard regression.
- set the train control argument to `ctrl_cv`.

```{r, echo = TRUE, eval = FALSE}
# Standard regression 
graduation_glm <- train(form = XX ~ .,
                        data = XX,
                        method = "XX",
                        trControl = XX)
```

```{r}
# Standard regression 
graduation_glm <- train(form = Grad.Rate ~ .,
                   data = college_train,
                   method = "glm",
                   trControl = ctrl_cv)
```

2. What were your final regression model coefficients?

```{r, echo = TRUE}
# Show final model
graduation_glm$finalModel
```

#### Ridge regression

3. Before you can fit a regularized regression model like ridge regression, you need to determine a vector of lambda penalty values that the cross validation procedure will evaluate. Using the code below, create a vector called `lambda_vec` containing 100 values spanning a range from very close to `0` up to `10`.

```{r, echo = TRUE}
# Vector of lambda values to try
lambda_vec <- 10 ^ (seq(-3, 1, length = 100))
```

4. Using `train()`, fit a ridge regression model predicting `Grad.Rate` as a function of all features. This time make use of the `tuneGrid`, which will take a `data.frame` specifying the sets of tuning parameters to consider during cross validation. In addition to `alpha = 0`, which specifies a ridge penalty, add `lambda = lambda_vec`. Also, don't forget to `"center"` and `"scale"` when using regularization.  

```{r, echo = TRUE, eval = FALSE}
# Ridge regression 
graduation_ridge <- train(form = XX ~ .,
                          data = XX,
                          method = "XX",
                          trControl = XX,
                          preProcess = c("XX", "XX"),          # Standardize
                          tuneGrid = data.frame(alpha = 0,     # Ridge penalty
                                                lambda = XX))  # Penalty weight
```

```{r}
# Ridge regression
graduation_ridge <- train(form = Grad.Rate ~ .,
                          data = college_train,
                          method = "glmnet",
                          trControl = ctrl_cv,
                          preProcess = c("center", "scale"),    # Standardise
                          tuneGrid = data.frame(alpha = 0,      # Ridge penalty
                                                lambda = lambda_vec)) # Penalty weight

```

5. Print your `graduation_ridge` object. Which lambda was selected as best performing?

```{r}
graduation_ridge
```

6. Plot your `graduation_ridge` object. What do you see? Does this match the plot match the value identified in the previous task?

```{r, echo = TRUE, eval = FALSE}
# Plot graduation_ridge object
plot(XX)
```

```{r}
plot(graduation_ridge)
```

7. What were your final regression model coefficients for the best lambda value? Find them by running the following code.

```{r, echo = TRUE}
# Get coefficients from best lambda value
coef(graduation_ridge$finalModel, 
     graduation_ridge$bestTune$lambda)
```

8. How do these coefficients compare to what you found in regular regression? Are they similar? Could the differences have something to do with the applied scaling?


9. Using `predict()` save the fitted values of `graduation_glm` object as `glm_fit`.

```{r}
# Save fitted value
glm_fit <- predict(graduation_glm)
```


#### Lasso regression

10. Now fit a lasso regression model predicting `Grad.Rate` as a function of all features. Set `alpha = 1` for the Lasso penalty and add `lambda = lambda_vec` as above. 


```{r, echo = TRUE, eval = FALSE}
# Lasso regression 
graduation_lasso <- train(form = XX ~ .,
                          data = XX,
                          method = "XX",
                          trControl = XX,
                          preProcess = c("XX", "XX"),         # Standardise
                          tuneGrid = data.frame(alpha = XX,   # Lasso penalty
                                                lambda = XX)) # Penalty weight
```

```{r}
# Lasso regression 
graduation_lasso <- train(form = Grad.Rate ~ .,
                          data = college_train,
                          method = "glmnet",
                          trControl = ctrl_cv,
                          preProcess = c("center", "scale"),   # Standardise
                          tuneGrid = data.frame(alpha = 1,     # Lasso penalty
                                                 lambda = lambda_vec)) # Penalty weight
```

11. Print your `graduation_lasso` object. Which lambda was selected as best performing?

```{r}
graduation_lasso
```

12. Plot your `graduation_lasso` object. What do you see? Does this match the plot match the value identified in the previous task?

```{r, echo = TRUE, eval = FALSE}
# Plot model object
plot(XX)
```

```{r}
plot(graduation_lasso)
```

13. What were your final regression model coefficients for the best lambda value? Find them by running the following code.

```{r, echo = TRUE}
# Get coefficients from best lambda value
coef(graduation_lasso$finalModel, 
     graduation_lasso$bestTune$lambda)
```

14. How do these coefficients compare to what you found for the regular and ridge regression? Have some features been set to 0?

#### Evaluate performance

15. Store the training data and test data criterion (`Grad.Rate`) as `criterion_train` and `criterion_test`. 

```{r}
# store criteria
criterion_train <- college_train$Grad.Rate
criterion_test <- college_test$Grad.Rate
```
s
16. Using `predict()`, save the fitted values of your models as  `glm_fit`, `ridge_fit`, and `lasso_fit`. 

```{r}
# store fitted values
glm_fit <- predict(graduation_glm)
ridge_fit <- predict(graduation_ridge)
lasso_fit <- predict(graduation_lasso)
```

17. Using `postResample` evaluate the fitting performance of your models. Which model has the best performance in fitting the training data? 

```{r}
# evaluate fit
postResample(pred = glm_fit, obs = criterion_train)
postResample(pred = ridge_fit, obs = criterion_train)
postResample(pred = lasso_fit, obs = criterion_train)
```

18. Using `predict()` and `newdata = college_test`, save the predicted values of your models as  `glm_pred`, `ridge_pred`, and `lasso_pred`. 

```{r}
# store predicted values
glm_pred <- predict(graduation_glm, newdata = college_test)
ridge_pred <- predict(graduation_ridge, newdata = college_test)
lasso_pred <- predict(graduation_lasso, newdata = college_test)
```

19. Using `postResample` evaluate the prediction performance of your models. Which model has the best performance in predicting the test data? Did the regularized regressions outperform the unregularized one? 

```{r}
# evaluate fit
postResample(pred = glm_pred, obs = criterion_test)
postResample(pred = ridge_pred, obs = criterion_test)
postResample(pred = lasso_pred, obs = criterion_test)
```

### E - Trees

#### Decision tree

1. It's time to see what parameter tuning can do for decision trees and random forests. To do this, first, determine a vector of possible values for the complexity parameter `cp` of decision trees. To this end, using the code below, create a vector called `cp_vec` which contains 100 values between 0 and .2.

```{r}
# Determine possible values for cp
cp_vec <- seq(from = 0, to = .2, length = 100)
```

2. Using `train()`, fit a decision tree model called `graduation_rpart` predicting `Grad.Rate`by all features. Again, assign a data frame to `tuneGrid` specifying the possible tuning parameters, i.e., `cp = cp_vec`.

```{r, echo = TRUE, eval = FALSE}
# Decision tree
graduation_rpart <- train(form = Grad.part ~ .,
                          data = XX,
                          method = "XX",
                          trControl = XX,
                          tuneGrid = data.frame(cp = XX))
```

```{r}
# Decision tree
graduation_rpart <- train(form = Grad.Rate ~ .,
                  data = college_train,
                  method = "rpart",
                  trControl = ctrl_cv,
                  tuneGrid = data.frame(cp = cp_vec))
```


3. Print your `graduation_rpart` object. Which `cp` was selected as best performing?

```{r}
graduation_rpart
```

4. Plot your `graduation_rpart` object. What do you see? Does this match the plot match the value identified in the previous task?

```{r}
plot(graduation_rpart)
```

5. Plot your final decision tree using the following code. Do you find the model sensible?

```{r, echo = TRUE}
# Visualise your trees
plot(as.party(graduation_rpart$finalModel)) 
```

6. How do the nodes in the tree compare to those in the ridge or lasso models?

#### Random forest

7. Now onto fitting a random forest. Using the code below, create a vector called `mtry_vec` containing values from 1 to 5, the tuning parameter candidates for our random forest.

```{r}
# mtry candidates
mtry_vec <- 1:5
```

8. Fit a random forest model predicting `Grad.Rate` as a function of all features. Make sure to use `mtry = mtry_vec` within the data frame specifying the `tuneGrid`. This one might take a bit longer than usual. 

```{r, echo = TRUE, eval = FALSE}
# Random forest
graduation_rf <- train(form = XX ~ .,
                   data = XX,
                   method = "XX",
                   trControl = XX,
                   tuneGrid = data.frame(mtry = XX))
```

```{r}
# Random forest
graduation_rf <- train(form = Grad.Rate ~ .,
                   data = college_train,
                   method = "rf",
                   trControl = ctrl_cv,
                   tuneGrid = data.frame(mtry = mtry_vec))
```

9. Print your `graduation_rf` object. What do you see? Which `mtry` was selected as best performing?

```{r}
graduation_rf
```

10. Plot your `graduation_rf` object. What do you see? Does this match the plot match the value identified in the previous task?

```{r}
plot(graduation_rf)
```

#### Evaluate performance

11. Using `predict()`, save the fitted values of your tree models as  `rpart_fit` and `rf_fit`. 

```{r}
# store fitted values
rpart_fit <- predict(graduation_rpart)
rf_fit <- predict(graduation_rf)
```

12. Using `postResample` evaluate the fitting performance of your models. Which model has the best performance in fitting the training data? If you like compare to the regression models of the previous section. 

```{r}
# evaluate fit
postResample(pred = rpart_fit, obs = criterion_train)
postResample(pred = rf_fit, obs = criterion_train)
```

13. Using `predict()` and `newdata = college_test`, save the predicted values of your models as `rpart_pred`, and `rf_pred`. 

```{r}
# store predicted values
rpart_pred <- predict(graduation_rpart, newdata = college_test)
rf_pred <- predict(graduation_rf, newdata = college_test)
```

14. Using `postResample` evaluate the prediction performance of your models. Which model has the best performance in predicting the test data? Did the tree models outperform the regularized regressions?

```{r}
# evaluate fit
postResample(pred = rpart_pred, obs = criterion_test)
postResample(pred = rf_pred, obs = criterion_test)
```


### X - Challenges: Explore tuning parameter grids

1. The name `tuneGrid` already suggests that one may want to vary multiple tuning parameters at the same time. A handy function helping in this is `expand.grid()`, which will produce all compbinations of values of the vectors supplied as its arguments. Try, e.g., `expand.grid(a = c(1, 2), b = c(2, 3, 4))`. The template below shows you how you can use `expand.grid()` to specify multiple tuning parameters at the same time.    

```{r, echo  =TRUE, eval = FALSE}
model <- train(form = XX ~ .,
               data = XX,
               method = "XX",
               trControl = XX,
               preProcess = c("XX", "XX"),         
               tuneGrid = expand.grid(parameter_1 = XX,    
                                      parameter_2 = XX)) 
```

2. Run and evaluate a regularized regression that uses cross validation to not only identify the best value for `lambda` but also the best value for `alpha`, e.g., using `alpha = c(0, .5, 1)`. This way you can let the procedure decide whether to use ridge, lasso or both. 

3. Run and evaluate a random forest while tuning not only `mtry` but also `ntree`, e.g., using `ntree = c(100,500,1000)`. Tip: avoid high values for `ntree` or `mtry`.

4. As done in the previous sessions try predicting `Private` rather than `Grad.Rate`. Note, this may require a different range of lambda values. You'll figure it out. 


## Examples

```{r, eval = FALSE, echo = TRUE}
# Model optimization with Regression

# Step 0: Load packages-----------
library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 
library(partykit)     # For decision trees
library(party)        # For decision trees

# Step 1: Load, clean, and explore data ----------------------

# training data
data_train <- read_csv("1_Data/diamonds_train.csv")

# test data
data_test <- read_csv("1_Data/diamonds_test.csv")

# Convert all characters to factor
#  Some ML models require factors
data_train <- data_train %>%
  mutate_if(is.character, factor)

data_test <- data_test %>%
  mutate_if(is.character, factor)

# Explore training data
data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Define criterion_train
criterion_train <- data_train$price
criterion_test <- data_test$price

# Step 2: Define training control parameters -------------

# Use 10-fold cross validation
ctrl_cv <- trainControl(method = "cv", 
                        number = 10) 

# Step 3: Train models: -----------------------------

# Normal Regression --------------------------
price_glm <- train(form = price ~ carat + depth + table + x + y,
                   data = data_train,
                   method = "glm",
                   trControl = ctrl_cv)


# Print key results
price_glm

# Coefficients
coef(price_glm$finalModel)

# Lasso --------------------------

# Vector of lambda values to try
lambda_vec <- 10 ^ seq(-3, 3, length = 100)

price_lasso <- train(form = price ~ carat + depth + table + x + y,
                   data = data_train,
                   method = "glmnet",
                   trControl = ctrl_cv,
                   preProcess = c("center", "scale"),  # Standardise
                   tuneGrid = data.frame(alpha = 1,  # Lasso
                                          lambda = lambda_vec))


# Print key results
price_lasso

# Plot regularisation parameter versus error
plot(price_lasso)

# Print best regularisation parameter
price_lasso$bestTune$lambda

# Get coefficients from best lambda value
coef(price_lasso$finalModel, 
     price_lasso$bestTune$lambda)

# Ridge --------------------------

# Vector of lambda values to try
lambda_vec <- 10 ^ seq(-3, 3, length = 100)

price_ridge <- train(form = price ~ carat + depth + table + x + y,
                     data = data_train,
                     method = "glmnet",
                     trControl = ctrl_cv,
                     preProcess = c("center", "scale"),  # Standardise
                     tuneGrid = data.frame(alpha = 0,     # Ridge penalty
                                            lambda = lambda_vec))

# Print key results
price_ridge

# Plot regularisation parameter versus error
plot(price_ridge)

# Print best regularisation parameter
price_ridge$bestTune$lambda

# Get coefficients from best lambda value
coef(price_ridge$finalModel, 
     price_ridge$bestTune$lambda)


# Decision Trees --------------------------

# Vector of cp values to try
cp_vec <- seq(0, .1, length = 100)

price_rpart <- train(form = price ~ carat + depth + table + x + y,
                  data = data_train,
                  method = "rpart",
                  trControl = ctrl_cv,
                  tuneGrid = data.frame(cp = cp_vec))

# Print key results
price_rpart

# Plot complexity parameter vs. error
plot(price_rpart)

# Print best complexity parameter
price_rpart$bestTune$cp

```



## Datasets

```{r, eval = TRUE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
```

|File  |Rows | Columns |
|:----|:-----|:------|
|[college_train.csv](https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_train.csv)| 50 | 20|
|[college_test.csv](https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv)| 213 | 20|
|[college_train.csv](https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_train.csv)| 500 | 18|
|[college_test.csv](https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv)| 277 | 18|
|[house_train.csv](https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/house_train.csv)| 5000 | 21|
|[house_test.csv](https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_test.csv)| 1000 | 21|


- The `college_train` and `college_test` data are taken from the `College` dataset in the `ISLR` package. They contain statistics for a large number of US Colleges from the 1995 issue of US News and World Report.

- The `house_train` and `house_test` data come from [https://www.kaggle.com/harlfoxem/housesalesprediction](https://www.kaggle.com/harlfoxem/housesalesprediction)


#### Variable description of `college_train` and `college_test`

| Name | Description |
|:-------------|:-------------------------------------|
| `Private` | A factor with levels No and Yes indicating private or public university. |
| `Apps` | Number of applications received.  |
| `Accept` | Number of applications accepted. |
| `Enroll` | Number of new students enrolled. |
| `Top10perc` | Pct. new students from top 10% of H.S. class. |
| `Top25perc` | Pct. new students from top 25% of H.S. class. |
| `F.Undergrad` | Number of fulltime undergraduates. |
| `P.Undergrad` | Number of parttime undergraduates. |
| `Outstate` | Out-of-state tuition. |
| `Room.Board` | Room and board costs. |
| `Books` | Estimated book costs. |
| `Personal` | Estimated personal spending. |
| `PhD` | Pct. of faculty with Ph.D.'s. |
| `Terminal` | Pct. of faculty with terminal degree. |
| `S.F.Ratio` | Student/faculty ratio. |
| `perc.alumni` | Pct. alumni who donate. |
| `Expend` | Instructional expenditure per student. |
| `Grad.Rate` | Graduation rate. |

#### Variable description of `house_train` and `house_test`

| Name | Description |
|:-------------|:-------------------------------------|
| `price` | Price of the house in $. |
| `bedrooms` | Number of bedrooms.  |
| `bathrooms` | Number of bathrooms. |
| `sqft_living` | Square footage of the home. |
| `sqft_lot` | Square footage of the lot. |
| `floors` | Total floors (levels) in house. |
| `waterfront` | House which has a view to a waterfront. |
| `view` | Has been viewed. |
| `condition` | How good the condition is (Overall). |
| `grade` | Overall grade given to the housing unit, based on King County grading system. |
| `sqft_above` | Square footage of house apart from basement. |
| `sqft_basement` | Square footage of the basement. |
| `yr_built` | Built Year. |
| `yr_renovated` | Year when house was renovated. |
| `zipcode` | Zip code. |
| `lat` | Latitude coordinate. |
| `long` | Longitude coordinate. |
| `sqft_living15` | Living room area in 2015 (implies some renovations). This might or might not have affected the lotsize area. |
| `sqft_lot15` | lot-size area in 2015 (implies some renovations). |

## Functions

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`caret`|`install.packages("caret")`|
|`partykit`|`install.packages("partykit")`|
|`party`|`install.packages("party")`|

### Functions

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `trainControl()`|`caret`|    Define modelling control parameters| 
| `train()`|`caret`|    Train a model|
| `predict(object, newdata)`|`stats`|    Predict the criterion values of `newdata` based on `object`|
| `postResample()`|`caret`|   Calculate aggregate model performance in regression tasks|
| `confusionMatrix()`|`caret`|   Calculate aggregate model performance in classification tasks| 

## Resources

<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf">
  <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="Trulli" style="width:70%"></a><br>
 <font style="font-size:10px"> from <a href= "https://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption">github.com/rstudio</a></font>
</figure>


