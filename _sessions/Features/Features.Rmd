---
title: "Features"
author: "Machine Learning with R<br>
  <a href='https://therbootcamp.github.io'>
    The R Bootcamp @ DHLab
  </a>
  <br>
  <a href='https://therbootcamp.github.io/ML-DHLab/'>
    <i class='fas fa-clock' style='font-size:.9em;'></i>
  </a>&#8239; 
  <a href='https://therbootcamp.github.io'>
    <i class='fas fa-home' style='font-size:.9em;' ></i>
  </a>&#8239;
  <a href='mailto:therbootcamp@gmail.com'>
    <i class='fas fa-envelope' style='font-size: .9em;'></i>
  </a>&#8239;
  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
    <i class='fab fa-linkedin' style='font-size: .9em;'></i>
  </a>"
date: "November 2020"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer">
  <span style="text-align:center">
    <span> 
      <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/>
    </span>
    <a href="https://therbootcamp.github.io/">
      <span style="padding-left:82px"> 
        <font color="#7E7E7E">
          www.therbootcamp.com
        </font>
      </span>
    </a>
    <a href="https://therbootcamp.github.io/">
      <font color="#7E7E7E">
       Machine Learning with R | November 2020
      </font>
    </a>
    </span>
  </div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
require(caret)
require(tidyverse)
# Code to knit slides
#bas = read_csv('1_Data/basel.csv')
bas = read_csv('1_Data/baselers.csv')

print2 <- function(x, nlines=10,...)
   cat(head(capture.output(print(x,...)), nlines), sep="\n")

sel = apply(bas, 1, function(x) any(is.na(unlist(x))))
bas = bas[!sel,]

bas = bas %>% 
  mutate_if(is.character, as.factor)

preprocesssing = preProcess(bas)
bas = predict(preprocesssing, bas)


bas <- bas %>%
  sample_n(1000)

bas <- bas %>% select_if(is.numeric)
basel = bas

fitControl_cv <- trainControl(
  method = "repeatedcv",
  number = 1,
  repeats = 1)

train_index = createDataPartition(bas$income, p = .8, list = FALSE)
bas_train = bas %>% slice(train_index)
bas_test = bas %>% slice(-train_index)

income_lm = train(income ~ . - id, 
           method = 'lm', 
           data = bas_train)

income_lm_short = train(income ~ age + food + alcohol + happiness + fitness + datause + tattoos + weight + children + fitness + height, 
           method = 'lm', 
           data = bas_train)

```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
```

```{r, echo = FALSE ,message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, 
                      message = FALSE, warning = FALSE, fig.align = 'center', dpi = 200)
library(tidyverse)
#library(basel)
library(ggthemes)
```



.pull-left45[

# Feature issues


<ul>
  <li class="m1"><span><b>Too many features</b></span></li>
  <ul class="level">
    <li><span>Curse of <high>dimensionality</high></span></li>
    <li><span>Feature <high>importance</high></span></li>
  </ul><br>
  <li class="m2"><span><b>Wrong features</b>
  <ul class="level">
    <li><span>Feature <high>scaling</high></span></li>
    <li><span>Feature <high>correlation</high></span></li>
    <li><span>Feature <high>quality</high></span></li>
  </ul>
  <li class="m3"><span><b>Create new features</b>
  <ul class="level">
    <li><span>Feature <high>engineering</high></span></li>
  </ul>
  </span></li>
</ul>

]

.pull-right45[

<br><br>

<p align="center">
<img src="image/dumbdata.png" height = 500px><br>
<font style="font-size:10px">from <a href="https://xkcd.com/1838/">xkcd.com</a></font>
</p>

]

---

# Curse of dimensionality

.pull-left35[

<ul>
  <li class="m1"><span><b>Density</b></span></li>
  <ul class="level">
    <li><span>The number of cases, that are necessary to <high>cover the data space</high> grows exponentially with the number of features.</span></li>
  </ul><br>
  <li class="m2"><span><b>Redundancy</b></span></li>
  <ul class="level">
    <li><span>Redundancy between the features grows with number, implying increased <high>uncertainty</high> in estimation.</span></li>
  </ul><br>
  <li class="m3"><span><b>Efficiency</b></span></li>
  <ul class="level">
    <li><span>The number of <high>parameters</high> grows with features requiring for <high>computational resources</high>.</span></li>
  </ul>
  </span></li>
</ul>

]

.pull-right6[

<br>

<p align="center">
<img src="image/cod.png"><br>
<font style="font-size:10px">from <a href="https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335?gi=6e6735e00188">medium.freecodecamp.org</a></font>
</p>



]

---

# How to reduce dimensionality?

.pull-left45[

<ul>
  <li class="m1"><span><b>Manual selection</b></span></li>
  <ul class="level">
    <li><span>Reduce features <high>manually</high> based on statistical or intuitive considerations..</span></li>
  </ul><br>
  <li class="m2"><span><b>Automatic selection</b></span></li>
  <ul class="level">
    <li><span>Reduce variables <high>automatically</high> using suitable ML algorithms, e.g., <mono>random forests</mono> or <mono>lasso</mono>, or feature selection algorithms, e.g., <mono>recursive feature selection</mono>.</span></li>
  </ul><br>
  <li class="m3"><span><b>Automatic reduction</b></span></li>
  <ul class="level">
    <li><span>Compress variables using <high>dimensionality reduction algorithms</high>, such as principal component analysis (PCA).</span></li>
  </ul>
  </span></li>
</ul>


]

.pull-right5[

<p align = "center">
<img src="image/highd.jpeg" height=350>
<font style="font-size:10px">from <a href="">Interstellar</a></font>
</p>

]

---

# Feature importance

.pull-left4[

<ul>
  <li class="m1"><span>Characterizes how much a <high>feature contributes</high> to the fitting/prediction performance. .</span></li><br>
  <li class="m2"><span>The metric is <high>model specific</high>, but typically <high>normalized</high> to <mono>[0, 100]</mono>.</span></li><br>
  <li class="m3"><span><b>Strategies</b></span></li>
  <ul class="level">
    <li><span>Single variable prediction (e.g., using LOESS, ROC) </span></li>
    <li><span>Accuracy loss from scrambling</span></li>
    <li><span>Random Forest importance</span></li>
    <li><span>etc.</span></li>
  </ul>
</ul>

]

.pull-right5[
```{r, eval = FALSE}
# plot variable importance for lm(income ~ .)
plot(varImp(income_lm))
```

```{r, echo = FALSE, fig.height=5.2, fig.width=8}
plot(varImp(income_lm),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```

]

---

# `varImp()`

.pull-left45[

<ul>
  <li class="m1"><span>Automatically selects <high>appropriate measure</high> of variable importance for a given algorithm.</span></li>
</ul>


```{r eval = FALSE}
varImp(income_lm)
```

```{r echo = F}
print2(varImp(income_lm_short), 11)
```

]

.pull-right5[
```{r, eval = FALSE}
# plot variable importance for lm(income ~ .)
plot(varImp(income_lm))
```

```{r, echo = FALSE, fig.height=5.2, fig.width=8}
plot(varImp(income_lm),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```

]

---

.pull-left35[

# Recursive feature selection

<ul>
  <li class="m1"><span><mono>rfe()</mono> uses <high>cross-validation</high> to select the best <i>n</i> freatures.</span></li><br>
  <li class="m2"><span>Algorithm</span></li>
  <ol>
    <li><span><high>Candidates</high>, e.g., <mono>n = [2, 3, 5, 10]</mono>.</span></li>
    <li><span><high>Resample</high> and split data.</span></li>
    <li><span>Evaluate <high>performance</high> for the best <mono>n</mono> features.</span></li>
    <li><span>Select best <mono>n</mono> on the basis of <high>aggregate performance</high>.</span></li>
  </ol>
</ul>

]

.pull-right55[

<br><br>

```{r, eval = F}
# Run feature elimination
rfe(x = ..., y = ..., 
    sizes = c(3,4,5,10), # feature set sizes
    rfeControl = rfeControl(functions = lmFuncs))
```

```{r, echo = F}
# Run feature elimination
out = rfe(x = bas_train %>% select(-income), 
    y = bas_train$income,
    sizes = c(3,4,5,10),   # Features set sizes should be considered
    rfeControl = rfeControl(
      functions = lmFuncs,
      verbose = FALSE))
print2(out, 20)
```

]

---

# Principal component analysis

.pull-left45[

<ul>
  <li class="m1"><span>The <high>go-to algorithm</high> for dimensionality reduction</span></li><br>
  <li class="m2"><span>Linear model (regression) represents features in a <high>new, smaller feature space</high>.</span></li><br>
  <li class="m3"><span>The new feature space explains <high>maximal variance</high> of the original features.</span></li>
</ul>

]


.pull-right45[

<p align = "center">
<img src="image/pca.png" height=350>
<font style="font-size:10px">from <a href="https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used
">blog.umetrics.com</a></font>
</p>


]

---

# Using `PCA`

.pull-left45[

```{r fig.height=5.1, fig.width=8, eval = F}
# train model WITHOUT PCA preprocessing
model = train(income ~ ., method = 'lm', 
           data = bas_train)

plot(varImp(model))
```

```{r fig.height=5.1, fig.width=8, echo = F}
# train model WITHOUT PCA preprocessing
model = train(income ~ . -id, method = 'lm', 
              data = bas_train)

plot(varImp(model, scale=F),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```


]

.pull-right45[

```{r fig.height=5.1, fig.width=8, eval = F}
# train model WITH PCA preprocessing
model = train(income ~ ., method = 'lm', 
              data = bas_train,
              preProc = c('pca'))
plot(varImp(model))
```

```{r fig.height=5.1, fig.width=8, echo = F}
# train model WITH PCA preprocessing
model = train(income ~ . -id, method = 'lm', 
              data = bas_train,
              preProc = c('pca'),
              trControl = trainControl(preProcOptions = list(thresh = 0.75)))
plot(varImp(model, scale=F),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```

]

---

# Other, easy feature problems

.pull-left45[

### Multi-collinearity

<ul>
  <li class="m1"><span><high>High feature correlations</high> mean that there is redundancy in the data, which can lead to less stable fits, uninterpretable variable importances, and worse predictions.</span></li>
</ul>


```{r}
# identify redundant variables
findCorrelation(cor(basel))

# remove from data
remove <- findCorrelation(cor(basel))
basel <- basel %>%
  select(-remove)

```

]

.pull-right45[

### Low variance 

<ul>
  <li class="m2"><span>Low variance variables add parameters, but <high>can hardly contribute to prediction</high> and are, thus, also redundant.</span></li>
</ul>


```{r}
# identify low variance variables
nearZeroVar(basel)
```


<ul>
  <li class="m3"><span>Unequal variance <high>breaks regularization</high> (L1, L2) and renders estimates difficult to interpret..</span></li>
</ul>

```{r, eval = F}
# standardize and center variables
train(..., preProc("center", "scale"))
```

]


---

# Difficult feature problems

<br>
.pull-left35[

<ul>
  <li class="m1"><span><b>Trivial Features</b></span></li>
  <ul>
    <li><span>Successful prediction not necessarily implies that a meaningful pattern has been detected.</span></li>
  </ul><br>
  <li class="m2"><span><b>Missing features</b></span></li>
  <ul>
    <li><span>Some problems are hard, requiring the engineering of new features.</span></li><br>
  </ul>
</ul>

]


.pull-right55[
<br>

<p align = "center">
<img src="image/here_to_help.png"><br>
<font style="font-size:10px">from <a href="https://xkcd.com/1831/">xkcd.com</a></font>
</p>


]

---

# Trivial features

.pull-left3[

<u><a href="https://www.gwern.net/Tanks">An urban myth?!</a></u>

"The Army trained a program to differentiate American tanks from Russian tanks with 100% accuracy. Only later did analysts realize that the American tanks had been photographed on a sunny day and the Russian tanks had been photographed on a cloudy day. The computer had learned to detect brightness."<br><br>
New York Times <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html" style="font-size:8px">[Full text]</a>

]

.pull-right6[

<p align = "center">
<img src="image/tank.jpg">
<font style="font-size:10px">from <a href="https://en.wikipedia.org/wiki/British_heavy_tanks_of_World_War_I#/media/File:Mark_I_series_tank.jpg">wikipedia.org</a></font>
</p>





]

---

# (Always!) missing features

.pull-left85[

<i>"…some machine learning projects succeed and some fail. What makes the difference? <high>Easily the most important factor is the features used</high>."</i>

[Pedro Domingos](https://en.wikipedia.org/wiki/Pedro_Domingos)

<br>

<i>"The algorithms we used are very standard for Kagglers. […] <high>We spent most of our efforts in feature engineering</high>. [...] We were also very careful to discard features likely to expose us to the risk of over-fitting our model."</i>

[Xavier Conort]()

<br>

<i>"Coming up with features is difficult, time-consuming, requires expert knowledge. <high>Applied machine learning is basically feature engineering</high>."</i>

[Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)

]

---

# Feature engineering

.pull-left45[

<br>

<i>“Feature engineering is the process of <high>transforming raw data</high> into features that <high>better represent the underlying problem</high> to the predictive models, resulting in improved model accuracy on unseen data.”</i>

[Jason Brownlee]()

<br>

<i>"...while avoiding the <high>curse of dimensionality</high>."</i>

[duw]()

]

.pull-right45[

<p align = "center">
<img src="image/albert.jpeg"><br>
<font style="font-size:10px">from <a href="http://www.open.edu/openlearncreate/mod/oucontent/view.php?id=80245&section=1">open.edu</a></font>
</p>


]

---

class: middle, center

<h1><a href="https://therbootcamp.github.io/ML-DHLab/_sessions/Features/Features_practical.html">Practical</a></h1>

